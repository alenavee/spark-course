{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "simpleData = [\n",
    "    (\"James\",\"Sales\",\"NY\",90000,34,10000), \n",
    "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000), \n",
    "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000), \n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000) \n",
    "  ]\n",
    "\n",
    "columns= [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "simpleData2 = [\n",
    "    (\"James\",\"Sales\",\"NY\",90000,34,10000), \n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000), \n",
    "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000), \n",
    "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000), \n",
    "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000) \n",
    "  ]\n",
    "columns2= [\"employee_name\", \"department\", \"state\", \"salary\", \"age\", \"bonus\"]\n",
    "\n",
    "df2 = spark.createDataFrame(data=simpleData2, schema = columns2)\n",
    "\n",
    "df2.printSchema()\n",
    "df2.show(truncate=False)\n",
    "\n",
    "unionDF = df.union(df2)\n",
    "unionDF.show(truncate=False)\n",
    "disDF = df.union(df2).distinct()\n",
    "disDF.show(truncate=False)\n",
    "\n",
    "unionAllDF = df.unionAll(df2)\n",
    "unionAllDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "simpleData3 = [\n",
    "    (\"Sales\",\"Jones\",\"NY\",90000,34,10000), \n",
    "    (\"Finance\",\"Maria\",\"CA\",90000,24,23000), \n",
    "    (\"Finance\",\"Jen\",\"NY\",79000,53,15000), \n",
    "  ]\n",
    "columns3= [\"department\", \"employee_name\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    "\n",
    "df3 = spark.createDataFrame(data = simpleData3, schema = columns3)\n",
    "\n",
    "print(\"df:\")\n",
    "df.show()\n",
    "print(\"df3:\")\n",
    "df3.show()\n",
    "\n",
    "unionDF = df.union(df3)\n",
    "unionDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "df.union(\n",
    "    df3.select(df.columns)\n",
    ").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace, col\n",
    "\n",
    "habrData\\\n",
    ".select(\"link\")\\\n",
    ".withColumn(\"company_id\", regexp_replace(col(\"link\"), \"(https://habr.com/ru/company/)|(/blog/[0-9]+/)|(https://habr.com/ru/post/[0-9]+/)\", \"\") )\\\n",
    ".show(50, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "arrayArrayData = [\n",
    "  (\"James\",[[\"Java\",\"Scala\",\"C++\"],[\"Spark\",\"Java\"]]),\n",
    "  (\"Michael\",[[\"Spark\",\"Java\",\"C++\"],[\"Spark\",\"Java\"]]),\n",
    "  (\"Robert\",[[\"CSharp\",\"VB\"],[\"Spark\",\"Python\"]])\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data=arrayArrayData, schema = ['name','subjects'])\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "from pyspark.sql.functions import explode\n",
    "df2 = df.select(df.name,explode(df.subjects).alias(\"exploded\"))\n",
    "df2.show()\n",
    "\n",
    "df2.select(df.name,explode(df2.exploded)).show(truncate=False)\n",
    "\n",
    "df2.select(df.name,explode(df2.exploded)).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "df.schema.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "arrayData = [\n",
    "        ('James',['Java','Scala'],{'hair':'black','eye':'brown'}),\n",
    "        ('Michael',['Spark','Java',None],{'hair':'brown','eye':None}),\n",
    "        ('Robert',['CSharp',''],{'hair':'red','eye':''}),\n",
    "        ('Washington',None,None),\n",
    "        ('Jefferson',['1','2'],{})\n",
    "        ]\n",
    "\n",
    "df = spark.createDataFrame(data=arrayData, schema = ['name', 'knownLanguages', 'properties'])\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "from pyspark.sql.functions import explode\n",
    "df3 = df.select(df.name,explode(df.properties))\n",
    "df3.printSchema()\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode_outer\n",
    "\"\"\" with array \"\"\"\n",
    "df.select(df.name,explode_outer(df.knownLanguages)).show()\n",
    "\"\"\" with map \"\"\"\n",
    "df.select(df.name,explode_outer(df.properties)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Window functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "simpleData = ((\"James\", \"Sales\", 3000), \n",
    "    (\"Michael\", \"Sales\", 4600),  \n",
    "    (\"Robert\", \"Sales\", 4100),   \n",
    "    (\"Maria\", \"Finance\", 3000),  \n",
    "    (\"James\", \"Sales\", 3000),    \n",
    "    (\"Scott\", \"Finance\", 3300),  \n",
    "    (\"Jen\", \"Finance\", 3900),    \n",
    "    (\"Jeff\", \"Marketing\", 3000), \n",
    "    (\"Kumar\", \"Marketing\", 2000),\n",
    "    (\"Saif\", \"Sales\", 4100) \n",
    "  )\n",
    " \n",
    "columns= [\"employee_name\", \"department\", \"salary\"]\n",
    "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, rank, dense_rank, percent_rank, ntile, cume_dist, round\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "windowSpec  = Window.partitionBy().orderBy(\"salary\")\n",
    "\n",
    "df\\\n",
    ".withColumn(\"row_number\", row_number().over(windowSpec)) \\\n",
    ".show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, rank, dense_rank, percent_rank, ntile, cume_dist, round\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "windowSpec  = Window.partitionBy(\"department\").orderBy(\"salary\")\n",
    "\n",
    "df\\\n",
    ".withColumn(\"row_number\", row_number().over(windowSpec)) \\\n",
    ".withColumn(\"rank\", rank().over(windowSpec)) \\\n",
    ".withColumn(\"dense_rank\", dense_rank().over(windowSpec)) \\\n",
    ".withColumn(\"percent_rank\", percent_rank().over(windowSpec)) \\\n",
    ".withColumn(\"ntile\", ntile(3).over(windowSpec)) \\\n",
    ".withColumn(\"cume_dist\", round(cume_dist().over(windowSpec), 2 )) \\\n",
    ".show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lag, lead\n",
    "\n",
    "df.withColumn(\"lag\", lag(\"salary\", 1).over(windowSpec)) \\\n",
    ".withColumn(\"lead\", lead(\"salary\", 2).over(windowSpec)) \\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.functions import col,avg,sum,min,max,row_number\n",
    "\n",
    "windowSpec     = Window.partitionBy(\"department\").orderBy(\"salary\")\n",
    "windowSpecAgg  = Window.partitionBy(\"department\")\n",
    "\n",
    "\n",
    "df.withColumn(\"row\", row_number().over(windowSpec)) \\\n",
    ".withColumn(\"avg_cum\", avg(col(\"salary\")).over(windowSpec)) \\\n",
    ".withColumn(\"avg\", avg(col(\"salary\")).over(windowSpecAgg)) \\\n",
    ".withColumn(\"sum_cum\", sum(col(\"salary\")).over(windowSpec)) \\\n",
    ".withColumn(\"sum\", sum(col(\"salary\")).over(windowSpecAgg)) \\\n",
    ".withColumn(\"min\", min(col(\"salary\")).over(windowSpecAgg)) \\\n",
    ".withColumn(\"max\", max(col(\"salary\")).over(windowSpecAgg)) \\\n",
    ".show()\n",
    "# .where(col(\"row\") == 1).select(\"department\", \"avg\", \"sum\", \"min\", \"max\") \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "spark.sql(\"with () as a, select * from \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "habrData = spark.read.option(\"header\", True).csv(\"/datasets/habr_data.csv\")\n",
    " \n",
    "from pyspark.sql.functions import udf, col, round\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "def mult(i):\n",
    "    return i * 2\n",
    "\n",
    "multUdf = udf(mult)\n",
    "\n",
    "habrData \\\n",
    ".select(\"rating\") \\\n",
    ".limit(10) \\\n",
    ".withColumn(\"udfString\", multUdf(col(\"rating\"))) \\\n",
    ".withColumn(\"udfInt\", multUdf(col(\"rating\").cast(IntegerType()))) \\\n",
    ".withColumn(\"round\", round(col(\"udfInt\")) ) \\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "spark.createDataFrame(([1], [2], [3]), schema=\"n INT\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "spark.createDataFrame(([1], [2], [3]), schema=\"n INT\") \\\n",
    ".selectExpr(\"n + 1\") \\\n",
    ".explain()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, col\n",
    "\n",
    "def plusOne(i):\n",
    "    return i + 1\n",
    "    \n",
    "plusOneUdf = udf(plusOne)\n",
    "\n",
    "spark.createDataFrame(([1], [2], [3]), schema=\"n INT\")\\\n",
    ".select(plusOneUdf(col(\"n\")).alias(\"plusOneUdf\"))\\\n",
    ".explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "spark.createDataFrame(([1], [2], [3]), schema=\"n INT\")\\\n",
    ".selectExpr(\"n + 1 as plusOne\")\\\n",
    ".where(\"plusOne = 2\")\\\n",
    ".explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "spark.createDataFrame(([1], [2], [3]), schema=\"n INT\")\\\n",
    ".withColumn(\"plusOne\", plusOneUdf(col(\"n\")))\\\n",
    ".where(\"plusOne = 2\")\\\n",
    ".explain()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "habrData = spark.read.option(\"header\", True).csv(\"/datasets/habr_data.csv\").cache()\n",
    " \n",
    "from pyspark.sql.functions import udf, col, when, expr\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "def mult(i):\n",
    "    return i * 2\n",
    "    \n",
    "def mult_nullsafe(i):\n",
    "    if i is None:\n",
    "        return 0\n",
    "    else:\n",
    "        return i * 2\n",
    "    \n",
    "# multUdf = udf(mult) \n",
    "multUdf = udf(mult_nullsafe)\n",
    "\n",
    "# .na.drop(\"all\")\\\n",
    "habrData\\\n",
    ".select(\"rating\")\\\n",
    ".withColumn(\"rating\", col(\"rating\").cast(IntegerType()))\\\n",
    ".withColumn(\"udfInt\", multUdf(col(\"rating\"))   )\\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, col\n",
    "\n",
    "\n",
    "spark.udf.register(\"mult_nullsafe\", mult_nullsafe)\n",
    "\n",
    "habrData \\\n",
    ".where(\"rating is not null\") \\\n",
    ".withColumn(\"rating\", col(\"rating\").cast(IntegerType())) \\\n",
    ".createOrReplaceTempView(\"habr_data\")\n",
    "\n",
    "spark.sql(\"select rating, mult_nullsafe(rating) plus_one from habr_data\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Integer type output\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "def square(i):\n",
    "    if i is None:\n",
    "        return 0\n",
    "    else:\n",
    "        return i * i\n",
    "    \n",
    "square_udf_int = udf(lambda z: square(z), IntegerType())\n",
    "\n",
    "habrData\\\n",
    ".select(\"rating\")\\\n",
    ".where(\"rating is not null\")\\\n",
    ".withColumn(\"rating\", col(\"rating\").cast(IntegerType()))\\\n",
    ".withColumn(\"square\", square_udf_int(col(\"rating\"))   )\\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "def square_list_float(x):\n",
    "    return [float(val)**2 for val in x]\n",
    "\n",
    "\n",
    "square_list_float_udf = udf(lambda y: square_list_float(y), ArrayType(FloatType()))\n",
    "\n",
    "\n",
    "cSchema = StructType([StructField(\"int_array\", ArrayType(IntegerType()))])\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    [[[1, 2]], [[3, 4, 5]], [[6, 7, 8, 9]]], schema=cSchema\n",
    ")\n",
    "\n",
    "df.printSchema()\n",
    "df.show()\n",
    "\n",
    "df\\\n",
    ".withColumn(\"square_list_float_udf\", square_list_float_udf(\"int_array\"))\\\n",
    ".show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "print(\"https://arena-hadoop.inno.tech:18088/proxy/\" + sc.applicationId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark 2.0.0",
   "language": "python",
   "name": "spark2"
  },
  "language_info": {
   "codemirror_mode": "text/python",
   "file_extension": ".py",
   "mimetype": "text/python",
   "name": "scala",
   "pygments_lexer": "python",
   "version": "3.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
