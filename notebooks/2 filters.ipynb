{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "schema = \"`Id` INT, `First` STRING, `Last` STRING, `Url` STRING, `Published` STRING, `Hits` INT, `Campaigns` ARRAY<STRING>\"\n",
    "\n",
    "# Create our static data\n",
    "data = [\n",
    "    [1, \"Jules\", \"Damji\", \"https://tinyurl.1\", \"1/4/2016\", 4535, [\"twitter\",\"LinkedIn\"]],\n",
    "    [2, \"Brooke\",\"Wenig\", \"https://tinyurl.2\", \"5/5/2018\", 8908, [\"twitter\", \"LinkedIn\"]],\n",
    "    [3, \"Denny\", \"Lee\", \"https://tinyurl.3\", \"6/7/2019\", 7659, [\"web\", \"twitter\", \"FB\", \"LinkedIn\"]],\n",
    "    [4, \"Tathagata\", \"Das\", \"https://tinyurl.4\", \"5/12/2018\", 10568, [\"twitter\", \"FB\"]],\n",
    "    [5, \"Matei\",\"Zaharia\", \"https://tinyurl.5\", \"5/14/2014\", 40578, [\"web\", \"twitter\", \"FB\", \"LinkedIn\"]],\n",
    "    [6, \"Reynold\", \"Xin\", \"https://tinyurl.6\", \"3/2/2015\", 25568, [\"twitter\", \"LinkedIn\"]]\n",
    "]\n",
    "\n",
    "# Create a DataFrame using the schema defined above\n",
    "blogs_df = spark.createDataFrame(data=data, schema=schema)\n",
    "# Show the DataFrame; it should reflect our table above\n",
    "blogs_df.show()\n",
    "# Print the schema used by Spark to process the DataFrame\n",
    "print(blogs_df.printSchema())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "blogs_df\\\n",
    ".selectExpr(\"*\" , \"concat(first, ' ', last) full_name\")\\\n",
    ".agg(sum(col(\"Hits\")).alias(\"Hits\"))\\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "events_df = spark.table(\"market.events\")\n",
    "\n",
    "grouped = events_df.groupBy(\"brand\").agg(f.count(\"*\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "grouped.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "blogs_df\\\n",
    ".where(\"hits > 10000\")\\\n",
    ".show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "sc.applicationId"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "\n",
    "What were all the different types of fire calls in 2018?\n",
    "What months within the year 2018 saw the highest number of fire calls?\n",
    "Which neighborhood in San Francisco generated the most fire calls in 2018?\n",
    "Which neighborhoods had the worst response times to fire calls in 2018?\n",
    "Which week in the year in 2018 had the most fire calls?\n",
    "Is there a correlation between neighborhood, zip code, and number of fire calls?\n",
    "How can we use Parquet files or SQL tables to store this data and read it back?\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "text/plain"
   },
   "source": [
    "%sh\n",
    "\n",
    "curl http://37.139.43.86/sf-fire-calls > sf-fire-calls.csv\n",
    "\n",
    "hdfs dfs -put sf-fire-calls.csv data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%sparkLocal.pyspark\n",
    "\n",
    "spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"data/sf-fire-calls.csv\") \\\n",
    ".write.parquet(\"data/sf-fire-calls.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "fireDF = spark.read.option(\"header\", True).load(\"data/sf-fire-calls.parquet\")\n",
    "fireDF.printSchema()\n",
    "fireDF.show(100, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "\n",
    "fireDF \\\n",
    ".withColumn(\"CallDate\", \n",
    "    f.to_timestamp(f.col(\"CallDate\"), 'dd/MM/yyyy'))\\\n",
    ".withColumn(\"CallYear\", f.year(\"CallDate\"))\\\n",
    ".where(\"CallYear = '2018'\") \\\n",
    ".select(\"CallType\")\\\n",
    ".distinct()\\\n",
    ".show(1000, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "fireDF\\\n",
    ".withColumn(\"CallDate\", f.to_timestamp(f.col(\"CallDate\"), 'dd/MM/yyyy'))\\\n",
    ".withColumn(\"CallMonth\", f.month(\"CallDate\"))\\\n",
    ".withColumn(\"CallYear\", f.year(\"CallDate\"))\\\n",
    ".where(\"CallYear = '2018'\") \\\n",
    ".groupBy(\"CallMonth\")\\\n",
    ".agg(\n",
    "    f.countDistinct(\"callNumber\").alias(\"call_count\")\n",
    "    )\\\n",
    ".orderBy(\"call_count\", ascending=False)\\\n",
    ".show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "(fireDF\n",
    ".groupBy(\"Neighborhood\")\n",
    ".agg(countDistinct(\"callNumber\").alias(\"call_count\"))\n",
    ".orderBy(\"call_count\", ascending=False)\n",
    ".show(1000, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "fireDF\\\n",
    ".where(\"lower(CallType) like '%fire%'\")\\\n",
    ".select(\"Neighborhood\", \"delay\", \"callType\")\\\n",
    ".orderBy(\"delay\", ascending=False)\\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "fireDF\\\n",
    ".where(\"lower(CallType) like '%fire%'\")\\\n",
    ".withColumn(\"CallDate\", f.to_timestamp(f.col(\"CallDate\"), 'dd/MM/yyyy'))\\\n",
    ".withColumn(\"CallWeek\", f.weekofyear(\"CallDate\"))\\\n",
    ".withColumn(\"CallYear\", f.year(\"CallDate\"))\\\n",
    ".where(\"CallYear = '2018'\") \\\n",
    ".groupBy(\"CallYear\", \"CallWeek\")\\\n",
    ".agg(\n",
    "    f.countDistinct(\"callNumber\").alias(\"call_count\")\n",
    "    )\\\n",
    ".orderBy(\"call_count\", ascending=False)\\\n",
    ".show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "df1 = spark.createDataFrame([\n",
    "    (1, \"andy\", 20, \"USA\"), \n",
    "    (2, \"jeff\", 23, \"China\"), \n",
    "    (3, \"james\", 18, \"USA\")]) \\\n",
    ".toDF(\"id\", \"name\", \"age\", \"country\")\n",
    "\n",
    "# Create udf create python lambda\n",
    "from pyspark.sql.functions import udf\n",
    "udf1 = udf(lambda e: e.upper())\n",
    "df2 = df1.select(udf1(df1[\"name\"]))\n",
    "df2.show()\n",
    "\n",
    "# UDF could also be used in filter, in this case the return type must be Boolean\n",
    "# We can also use annotation to create udf\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "@udf(returnType=BooleanType())\n",
    "def udf2(e):\n",
    "    if e >= 20:\n",
    "        return True;\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "df3 = df1.filter(udf2(df1[\"age\"]))\n",
    "df3.show()\n",
    "\n",
    "# UDF could also accept more than 1 argument.\n",
    "udf3 = udf(lambda e1, e2: e1 + \"_\" + e2)\n",
    "df4 = df1.select(udf3(df1[\"name\"], df1[\"country\"]).alias(\"name_country\"))\n",
    "df4.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "df1 = spark.createDataFrame([\n",
    "    (1, \"andy\", 20, 1), \n",
    "    (2, \"jeff\", 23, 2), \n",
    "    (3, \"james\", 18, 3)])\n",
    "    .toDF(\"id\", \"name\", \"age\", \"c_id\")\n",
    "    \n",
    "df1.show()\n",
    "\n",
    "df2 = spark.createDataFrame([(1, \"USA\"), (2, \"China\")]).toDF(\"c_id\", \"c_name\")\n",
    "df2.show()\n",
    "\n",
    "# You can just specify the key name if join on the same key\n",
    "df3 = df1.join(df2, on=[\"c_id\"], how=\"inner\")\n",
    "df3.show()\n",
    "\n",
    "# Or you can specify the join condition expclitly in case the key is different between tables\n",
    "df4 = df1.join(df2, df1[\"c_id\"] == df2[\"c_id\"] )\n",
    "df4.show()\n",
    "\n",
    "# You can specify the join type afte the join condition, by default it is inner join\n",
    "df5 = df1.join(df2, df1[\"c_id\"] == df2[\"c_id\"], \"left_outer\")\n",
    "df5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "df1 = spark.createDataFrame([(\"andy\", 20, 1, 1), (\"jeff\", 23, 1, 2), (\"james\", 12, 2, 2)]).toDF(\"name\", \"age\", \"key_1\", \"key_2\")\n",
    "df1.show()\n",
    "\n",
    "df2 = spark.createDataFrame([(1, 1, \"USA\"), (2, 2, \"China\")]).toDF(\"key_1\", \"key_2\", \"country\")\n",
    "df2.show()\n",
    "\n",
    "# Join on 2 fields: key_1, key_2\n",
    "\n",
    "# You can pass a list of field name if the join field names are the same in both tables\n",
    "df3 = df1.join(df2, [\"key_1\", \"key_2\"])\n",
    "df3.show()\n",
    "\n",
    "# Or you can specify the join condition expclitly in case when the join fields name is differetnt in the two tables\n",
    "df4 = df1.join(df2, (df1[\"key_1\"] == df2[\"key_1\"]) & (df1[\"key_2\"] == df2[\"key_2\"]))\n",
    "df4.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "df1 = spark.createDataFrame([\n",
    "        (1, \"andy\", 20, \"USA\"), \n",
    "        (2, \"jeff\", 23, \"China\"), \n",
    "        (3, \"james\", 18, \"USA\")]) \\\n",
    "    .toDF(\"id\", \"name\", \"age\", \"country\")\n",
    "    \n",
    "df1.createOrReplaceTempView(\"people\")\n",
    "\n",
    "df2 = spark.\n",
    "(\"select name, age from people\")\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark 2.0.0",
   "language": "python",
   "name": "spark2"
  },
  "language_info": {
   "codemirror_mode": "text/python",
   "file_extension": ".py",
   "mimetype": "text/python",
   "name": "scala",
   "pygments_lexer": "python",
   "version": "3.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
