{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "format": "text/plain"
   },
   "source": [
    "%conf\n",
    "\n",
    "spark.executor.memory=512mb\n",
    "spark.executor.num=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "spark.table(\"hw_4.events_full\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "sample_very_big_df = spark.table(\"hw_4.sample_very_big\")\n",
    "sample_big_df = spark.table(\"hw_4.sample_big\")\n",
    "\n",
    "sample_very_big_df.join(sample_big_df, \"event_id\", \"inner\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "spark.table(\"hw_4.sample_very_big\") \\\n",
    ".write.mode(\"overwrite\").bucketBy(10, \"event_id\") \\\n",
    ".saveAsTable(\"hw_4.sample_very_big_bucket\")\n",
    "\n",
    "\n",
    "spark.table(\"hw_4.sample_big\") \\\n",
    ".write.mode(\"overwrite\").bucketBy(10, \"event_id\") \\\n",
    ".saveAsTable(\"hw_4.sample_big_bucket\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "print(\"HTTPS://arena-hadoop.inno.tech:18088/proxy/\" + sc.applicationId + \"/jobs/\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "text/plain"
   },
   "source": [
    "%sh\n",
    "\n",
    "hdfs dfs -ls -h /apps/hive/warehouse/hw_4.db/sample_big\n",
    "\n",
    "hdfs dfs -ls -h /apps/hive/warehouse/hw_4.db/sample_very_big\n",
    "\n",
    "hdfs dfs -ls -h /apps/hive/warehouse/hw_4.db/sample_very_big_bucket\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "sample_very_big_df = spark.table(\"hw_4.sample_very_big_bucket\")\n",
    "sample_big_df = spark.table(\"hw_4.sample_big_bucket\")\n",
    "\n",
    "sample_very_big_df.join(sample_big_df, \"event_id\", \"inner\").count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, lit, col\n",
    "\n",
    "skew_column = when(col(\"id\") < 900, lit(0)).otherwise(lit(1)).alias(\"skew_column\")\n",
    "\n",
    "skewed_df = spark.range(1000).withColumn(\"skew\", skew_column).repartition(10, col(\"skew\"))\n",
    "\n",
    "skewed_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "def print_parts(df):\n",
    "    ret = df.rdd.mapPartitions(lambda x: [len(list(x))]).collect()\n",
    "    print(ret)\n",
    "    \n",
    "print_parts(skewed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# здесь мы передаем только новое количество партиций и Spark выполнит RoundRobinPartitioning\n",
    "\n",
    "balanced_df = skewed_df.repartition(11)\n",
    "print_parts(balanced_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# здесь мы добавляем к числу партиций колонку, по которой необходимо сделать репартиционирование,\n",
    "# поэтому Spark выполнит HashPartitioning\n",
    "\n",
    "balanced_df = skewed_df.repartition(20, col(\"id\"))\n",
    "print_parts(balanced_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Добавление соли\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "text/plain"
   },
   "source": [
    "%sh\n",
    "\n",
    "curl https://github.com/datasets/airport-codes/raw/master/data/airport-codes.csv  > airport-codes.csv\n",
    "\n",
    "hdfs dfs -put airport-codes.csv data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%sparkLocal.pyspark\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "df = spark.read.format(\"csv\").options(header=True, inferSchema=True).load(\"data/airport-codes.csv\")\n",
    "\n",
    "df.groupBy(f.col(\"type\")).count().orderBy(f.col(\"count\").desc()).show(30, False)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import collect_list, col\n",
    "\n",
    "skew_grouped = df.groupBy(col(\"type\")).agg(collect_list(col(\"ident\")).alias(\"ids\"))\n",
    "skew_grouped.show(20, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку при вычислении агрегата происходит неявный HashPartitioning по ключу (ключам) агрегата, то при выполнении определенных условий происходит нехватка памяти на воркере, которую нельзя исправить, не изменив подход к построению агрегата.\n",
    "\n",
    "Один из вариантов устранение - соление ключей:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "salt = expr(\"\"\"pmod(round(rand() * 100, 0), 10)\"\"\").cast(\"integer\")\n",
    "salted = df.withColumn(\"salt\", salt)\n",
    "salted.select(col(\"type\"), col(\"ident\"), col(\"salt\")).sample(0.1).show(20, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это позволяет нам существенно снизить объем данных в каждой партиции (30к vs 3к):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "salted.groupBy(col(\"type\"), col(\"salt\")).count().orderBy(col(\"count\").desc()).show(20, False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это позволяет нам посчитать требуемый агрегат более оптимальным путем, не смотря на появление второго агрегата:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "salted \\\n",
    "    .groupBy(col(\"type\"), col(\"salt\")).agg(collect_list(col(\"ident\")).alias(\"ids\")) \\\n",
    "    .groupBy(col(\"type\")).agg(collect_list(col(\"ids\")).alias(\"ids\")) \\\n",
    "    .show(20, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Кеширование\n",
    "По умолчанию при применении каждого действия Spark пересчитывает весь граф, что может негативно сказать на производительности приложения. Для демонстрации возьмем датасет Airport Codes\n",
    "Посчитаем несколько действий. Несмотря на то, что only_ru является общим для всех действий, он пересчитывается при вызове каждого действия."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "only_ru = df.filter((col(\"iso_country\") == \"RU\") & (col(\"elevation_ft\") > 1000))\n",
    "only_ru.show(1, 50, True)\n",
    "\n",
    "only_ru.count()\n",
    "only_ru.collect()\n",
    "only_ru.groupBy(col(\"municipality\")).count().orderBy(col(\"count\").desc()).na.drop(\"any\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для решения этой проблемы следует использовать методы cache, либо persist. Данные методы сохраняют состояние графа после первого действия, и следующие обращаются к нему. Разница между методами заключается в том, что persist позволяет выбрать, куда сохранить данные, а cache использует значение по умолчанию. В текущей версии Spark это StorageLevel.MEMORY_ONLY. Важно помнить, что данный кеш не предназначен для обмена данными между разными Spark приложения - он является внутренним для приложения. После того, как работа с данными окончена, необходимо выполнить unpersist для очистки памяти\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "only_ru = df.filter((col(\"iso_country\") == \"RU\") & (col(\"elevation_ft\") > 1000))\n",
    "only_ru.cache()\n",
    "only_ru.show(1, 50, True)\n",
    "only_ru.count()\n",
    "only_ru.collect()\n",
    "only_ru.groupBy(col(\"municipality\")).count().orderBy(col(\"count\").desc()).na.drop(\"any\").show()\n",
    "# only_ru.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "only_ru.groupBy(col(\"municipality\")).count().orderBy(col(\"count\").desc()).na.drop(\"any\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Планы выполнения задач\n",
    "\n",
    "Любой job в Spark SQL имеет под собой план выполнения, кототорый генерируется на основе написанно запроса. План запроса содержит операторы, которые затем превращаются в Java код. Поскольку одну и ту же задачу в Spark SQL можно выполнить по-разному, полезно смотреть в планы выполнения, чтобы, например:\n",
    "\n",
    "    убрать лишние shuffle\n",
    "    убедиться, чтот тот или иной оператор будет выполнен на уровне источника, а не внутри Spark\n",
    "    понять, как будет выполнен join\n",
    "\n",
    "Планы выполнения доступны в двух видах:\n",
    "\n",
    "    метод explain() у DF\n",
    "    на вкладке SQL в Spark UI\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df.filter(col(\"type\") == \"small_airport\").explain(extended=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выполним агрегацию и проверим план выполнения. В нем появляется три оператора: 2 HashAggregate и Exchange hashpartitioning.\n",
    "\n",
    "Первый HashAggregate содержит функцию partial_count(1). Это означает, что внутри каждого воркера произойдет подсчет строк по каждому ключу. Затем происходит shuffle по ключу агрегата, после которого выполняется еще один HashAggregate с функцией count(1). Использование двух HashAggregate позволяет сократить количество передаваемых данных по сети.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df.filter(col(\"type\") == \"small_airport\").groupBy(col(\"iso_country\")).count().explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оптимизация соединений и группировок\n",
    "\n",
    "При выполнении join двух DF важно следовать рекомендациям:\n",
    "\n",
    "- фильтровать данные до join'а\n",
    "- использовать equ join\n",
    "- если можно путем увеличения количества данных применить equ join вместо non-equ join'а, то делать именно так\n",
    "- всеми силами избегать cross-join'ов\n",
    "- если правый DF помещается в памяти worker'а, использовать broadcast()\n",
    "\n",
    "Виды джойнов\n",
    "\n",
    "- BroadcastHashJoin\n",
    "-- equ join\n",
    "-- broadcast\n",
    "- SortMergeJoin\n",
    "-- equ join\n",
    "-- sortable keys\n",
    "- BroadcastNestedLoopJoin\n",
    "-- non-equ join\n",
    "-- using broadcast\n",
    "- CartesianProduct\n",
    "-- non-equ join\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "left = df.select(col(\"type\"), col(\"ident\"), col(\"iso_country\")).alias(\"left\").localCheckpoint()\n",
    "right = df.groupBy(col(\"type\")).count().alias(\"right\").localCheckpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BroadcastHashJoin¶\n",
    "\n",
    "- работает, когда условие - равенство одного или нескольких ключей\n",
    "- работает, когда один из датасетов небольшой и полностью вмещается в память воркера\n",
    "- оставляет левый датасет как есть\n",
    "- копирует правый датасет на каждый воркер\n",
    "- составляет hash map из правого датасета, где ключ - кортеж из колонок в условии соединения\n",
    "- итерируется по левому датасета внутри каждой партиции и проверяет наличие ключей в HashMap\n",
    "- может быть автоматически использован, либо явно через broadcast(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "result = left.join(broadcast(right), \"type\", \"inner\")\n",
    "# result = left.join(right, \"type\", \"inner\")\n",
    "\n",
    "result.explain()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SortMergeJoin\n",
    "+ работает, когда ключи соединения в обоих датасета являются сортируемыми\n",
    "+ репартиционирует оба датасета в 200 партиций по ключу (ключам) соединения\n",
    "+ сортирует партиции каждого из датасетов по ключу (ключам) соединения\n",
    "+ Используя сравнение левого и правого ключей, обходит каждую пару партиций и соединяет строки с одинаковыми ключами\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "\n",
    "result = left.join(right, \"type\", \"inner\")\n",
    "\n",
    "result.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BroadcastNestedLoopJoin\n",
    "+ работает, когда один из датасетов небольшой и полностью вмещается в память воркера\n",
    "+ оставляет левый датасет как есть\n",
    "+ копирует правый датасет на каждый воркер\n",
    "+ проходится вложенным циклом по каждой партиции левого датасета и копией правого датасета и проверяет условие\n",
    "+ может быть автоматически использован, либо явно через `broadcast(df)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "\n",
    "result = left.join(broadcast(right), left[\"type\"] != right[\"type\"], \"inner\")\n",
    "\n",
    "result.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CartesianProduct\n",
    "+ Создает пары из каждой партиции левого датасета с каждой партицией правого датасета, релоцирует каждую пару на один воркер и проверяет условие соединения\n",
    "+ на выходе создает N*M партиций\n",
    "+ работает медленнее остальных и часто приводит к ООМ воркеров\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "\n",
    "result = left.join(right, left[\"type\"] != right[\"type\"], \"inner\")\n",
    "\n",
    "result.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Снижение количества shuffle\n",
    "\n",
    "В ряде случаев можно уйти от лишних shuffle операций при выполнении соединения. Для этого оба DF должны иметь одинаковое партиционирование - одинаковое количество партиций и ключ партиционирования, совпадающий с ключом соединения.\n",
    "\n",
    "Разница между планами выполнения будет хорошо видна в Spark UI на графе выполнения в Jobs и плане выполнения в SQL\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "df.unpersist()\n",
    "\n",
    "left = df\n",
    "right = df.groupBy(col(\"type\")).count()\n",
    "joined = left.join(right, \"type\")\n",
    "joined.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "df_rep = df.repartition(200, col(\"type\"))\n",
    "\n",
    "left = df_rep\n",
    "right = df_rep.groupBy(col(\"type\")).count()\n",
    "\n",
    "joined = left.join(right, \"type\")\n",
    "joined.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оптимизатор запросов Catalyst\n",
    "Catalyst выполняет оптимизацию запросов с целью ускорения их выполнения и применяет следующие методы:\n",
    " + Column projection\n",
    " + Partition pruning\n",
    " + Predicate pushdown\n",
    " + Constant folding\n",
    " \n",
    " Подготовим датасет для демонстрации работы Catalyst:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "df \\\n",
    "    .write \\\n",
    "    .format(\"parquet\") \\\n",
    "    .partitionBy(\"iso_country\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(\"/tmp/airports.parquet\") \n",
    "\n",
    "airports = spark.read.parquet(\"/tmp/airports.parquet\")\n",
    "airports"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "text/plain"
   },
   "source": [
    "%sh\n",
    "hdfs dfs -ls /tmp/airports.parquet\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "text/plain"
   },
   "source": [
    "%sh\n",
    "\n",
    "hdfs dfs -ls /tmp/airports.parquet/iso_country=AD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "spark.read.parquet(\"/tmp/airports.parquet/iso_country=AD/part-00000-ef581cca-e2a2-4a9e-b75f-d3cf7387ed30.c000.snappy.parquet\").printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column projection\n",
    "Данный механизм позволяет избегать вычитывания ненужных колонок при работе с источниками\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "selected = airports.select(col(\"ident\"))\n",
    "selected.cache()\n",
    "selected.count()\n",
    "selected.unpersist()\n",
    "selected.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "selected = airports\n",
    "selected.cache()\n",
    "selected.count()\n",
    "selected.unpersist()\n",
    "selected.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partition pruning\n",
    "Данный механизм позволяет избежать чтения ненужных партиций\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "filtered = airports.filter(col(\"iso_country\") == \"RU\")\n",
    "filtered.count()\n",
    "filtered.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicate pushdown\n",
    "Данный механизм позволяет \"протолкнуть\" условия фильтрации данных на уровень datasource\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "filtered = airports.filter(col(\"iso_region\") == \"RU\")\n",
    "filtered.count()\n",
    "filtered.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplify casts\n",
    "Данный механизм убирает ненужные `cast`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "result = spark.range(10).select(col(\"id\").cast(\"long\"))\n",
    "result.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "result = spark.range(10).select(col(\"id\").cast(\"int\").cast(\"long\"))\n",
    "result.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constant folding\n",
    "Данный механизм сокращает количество констант, используемых в физическом плане\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "result = spark.range(10).select((lit(3) > lit(0)).alias(\"foo\"))\n",
    "result.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit, col\n",
    "\n",
    "result = spark.range(10).select((col(\"id\") >  lit(0)).alias(\"foo\"))\n",
    "result.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine filters\n",
    "Данный механизм объединяет фильтры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "result = spark.range(10).filter(col(\"id\") > 0) \\\n",
    "    .filter(col(\"id\") != 5) \\\n",
    "    .filter(col(\"id\") < 10)\n",
    "    \n",
    "result.explain(True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark 2.0.0",
   "language": "python",
   "name": "spark2"
  },
  "language_info": {
   "codemirror_mode": "text/python",
   "file_extension": ".py",
   "mimetype": "text/python",
   "name": "scala",
   "pygments_lexer": "python",
   "version": "3.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
