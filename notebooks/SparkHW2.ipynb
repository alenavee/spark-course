{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "\n",
    "events_df = spark.table(\"market.events\")\n",
    "\n",
    "events_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "events_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "#pyspark.sql.functions.when\n",
    "\n",
    "events_df \\\n",
    ".filter(\"event_type == 'purchase'\") \\\n",
    ".withColumn(\"isApple\", f.when(f.col(\"brand\")==\"apple\", True).otherwise(False)) \\\n",
    ".groupby(\"isApple\") \\\n",
    ".count() \\\n",
    ".show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# to_timestamp, date_trunc\n",
    "\n",
    "hour_trunc = f.date_trunc(\"hour\", f.to_timestamp(f.col(\"event_time\"), 'yyyy-MM-dd HH:mm:ss'))\n",
    "\n",
    "z.show(\n",
    "    events_df\\\n",
    "    .filter(\"event_type=='purchase'\")\\\n",
    "    .withColumn('hour', hour_trunc)\\\n",
    "    .withColumnRenamed(\"price\", \"profit\") \\\n",
    "    .groupby('hour')\\\n",
    "    .agg(\n",
    "        {'profit':'sum', 'hour':'count'}\n",
    "        )\n",
    "    .orderBy('hour')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "hour = f.hour(f.to_timestamp(f.col('event_time'), 'yyyy-MM-dd HH:mm:ss'))\n",
    "\n",
    "days = events_df\\\n",
    "    .filter(\"event_type == 'purchase'\")\\\n",
    "    .withColumn('day', f.to_timestamp(f.col('event_time'), 'yyyy-MM-dd'))\\\n",
    "    .select('day')\\\n",
    "    .distinct().count()\n",
    "\n",
    "z.show(\n",
    "    events_df\\\n",
    "    .filter(\"event_type == 'purchase'\")\\\n",
    "    .withColumn('hours', hour)\\\n",
    "    .groupBy('hours')\\\n",
    "    .count()\\\n",
    "    .withColumn('mean', f.col('count') / days)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "days = events_df\\\n",
    "    .filter(\"event_type == 'view'\")\\\n",
    "    .withColumn('day', f.to_timestamp(f.col('event_time'), 'yyyy-MM-dd'))\\\n",
    "    .select('day')\\\n",
    "    .distinct().count()\n",
    "\n",
    "z.show(\n",
    "    events_df\\\n",
    "    .filter(\"event_type == 'view'\")\\\n",
    "    .withColumn('hours', hour)\\\n",
    "    .groupBy('hours')\\\n",
    "    .count()\\\n",
    "    .withColumn('mean', f.col('count') / days)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark 2.0.0",
   "language": "python",
   "name": "spark2"
  },
  "language_info": {
   "codemirror_mode": "text/python",
   "file_extension": ".py",
   "mimetype": "text/python",
   "name": "scala",
   "pygments_lexer": "python",
   "version": "3.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
