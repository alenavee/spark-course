{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "\n",
    "events_df = spark.table(\"market.events\").where(\"date < '2019-11-02'\")\n",
    "\n",
    "events_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "\n",
    "split_cols = f.split('category_code', '\\.')\n",
    "\n",
    "arr_cols = [split_cols[i].alias('category_' + str(i+1)) for i in range(3)]\n",
    "\n",
    "category_df = events_df.select(\"*\", *arr_cols)\n",
    "category_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank, col\n",
    "\n",
    "\n",
    "window = Window.partitionBy(category_df['category_2']).orderBy(f.col('views').desc())\n",
    "\n",
    "category_df.filter(\"event_type = 'view'\") \\\n",
    ".filter(col(\"brand\").isNotNull()) \\\n",
    ".groupby('category_1', 'category_2', 'brand') \\\n",
    ".agg(f.count('*').alias(\"views\")) \\\n",
    ".withColumn(\"rank\", rank().over(window)) \\\n",
    ".filter(col('rank') <= 3)  \\\n",
    ".show(100)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "sch=ArrayType(StringType());\n",
    "\n",
    "# скачать датасет: http://37.139.43.86/tracks \n",
    "\n",
    "# важно что разделитель ', ' с пробелом, иначе пробелы добавятся в значения\n",
    "tracks = spark.read.option(\"header\", \"true\") \\\n",
    "        .option(\"escape\", '\"') \\\n",
    "        .option(\"InferSchema\", \"true\") \\\n",
    "        .csv(\"data/tracks.csv\") \\\n",
    "        .withColumn(\"release_year\", f.substring(\"release_date\", 1, 4).cast(IntegerType())) \\\n",
    "        .withColumn(\"array_artist\", f.split(f.regexp_replace(f.col(\"artists\"), \"[\\]\\[\\']\", \"\"),\", \")) \\\n",
    "        .cache() #выделяем год в отдельную колонку и преобразуем колонку с артистами в массив\n",
    "\n",
    "tracks_exp = tracks.select(  \n",
    "                            \"name\", \n",
    "                            \"popularity\",\n",
    "                            \"danceability\",\n",
    "                            \"energy\",\n",
    "                            \"speechiness\",\n",
    "                            \"acousticness\",\n",
    "                            \"liveness\",\n",
    "                            \"valence\",\n",
    "                            \"release_year\",\n",
    "                            \"artists\",\n",
    "                            f.explode(f.col(\"array_artist\") ).alias(\"name_artist\")\n",
    "                        ) #создаем отдельную таблицу с развернутым массивом артистов\n",
    "                        \n",
    "tracks_exp.printSchema()\n",
    "\n",
    "spark.sql(\"create database hw_3\")\n",
    "tracks_exp.write.mode(\"overwrite\").saveAsTable(\"hw_3.tracks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "tracks = spark.table(\"hw_3.tracks\")\n",
    "tracks.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "window = Window.partitionBy(f.col(\"release_year\")).orderBy(f.col('popularity').desc())\n",
    "\n",
    "top_100_yearly = tracks\\\n",
    ".select('name_artist', 'name', 'release_year', 'popularity') \\\n",
    ".withColumn(\"rank\", rank().over(window))\\\n",
    ".filter((col('rank') <= 100) & (col('popularity') > 0)) \n",
    "\n",
    "top_100_yearly.groupby(\"name_artist\") \\\n",
    ".count() \\\n",
    ".orderBy(f.col('count').desc()) \\\n",
    ".show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "top_100_yearly.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "top_100_yearly.dropDuplicates(['name_artist', 'release_year'])\\\n",
    ".groupBy('name_artist')\\\n",
    ".count()\\\n",
    ".orderBy(f.col(\"count\").desc())\\\n",
    ".show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "def max_consecutive(years_list):\n",
    "    years_list.sort()\n",
    "    count = 1\n",
    "    max_count = 1\n",
    "    con_list = []\n",
    "    for i in range(len(years_list)-1):\n",
    "        if years_list[i] + 1 == years_list[i+1]:\n",
    "            count+=1\n",
    "        else:\n",
    "            max_count = max(max_count, count)\n",
    "            count = 1\n",
    "    max_count = max(max_count, count)\n",
    "    return(max_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "udf_years=udf(lambda x: max_consecutive(x), IntegerType())\n",
    "\n",
    "top_100_yearly.groupby('name_artist')\\\n",
    ".agg(f.collect_set(col('release_year')).alias('years_list'))\\\n",
    ".select('name_artist', udf_years(col('years_list')).alias('count'))\\\n",
    ".orderBy(col('count').desc())\\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark 2.0.0",
   "language": "python",
   "name": "spark2"
  },
  "language_info": {
   "codemirror_mode": "text/python",
   "file_extension": ".py",
   "mimetype": "text/python",
   "name": "scala",
   "pygments_lexer": "python",
   "version": "3.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
